{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural style transfer consists in applying the \"style\" of a reference image to a target image, while conserving the \"content\" of the target image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a loss function to specify what we want to achieve: conserve the \"content\" of the original image, while adopting the \"style\" of the reference image. If we were able to mathematically define content and style, then an appropriate loss function to minimize would be the following:\n",
    "\n",
    "loss = distance(style(reference_image) - style(generated_image)) +\n",
    "       distance(content(original_image) - content(generated_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- distance is a norm function such as the L2 norm\n",
    "- content is a function that takes an image and computes a representation of its \"content\"\n",
    "- style is a function that takes an image and computes a representation of its \"style\".\n",
    "\n",
    "Minimizing this loss would cause style(generated_image) to be close to style(reference_image), while content(generated_image) would be close to content(generated_image), thus achieving style transfer as we defined it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pre-trained convnet to define a loss that will:\n",
    "\n",
    "- Preserve content by maintaining similar high-level layer activations between the target content image and the generated image. The convnet should \"see\" both the target image and the generated image as \"containing the same things\".\n",
    "\n",
    "- Preserve style by maintaining similar correlations within activations for both low-level layers and high-level layers. Indeed, feature correlations capture textures: the generated and the style reference image should share the same textures at different spatial scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "target_image_path = 'Pictures/dog.jpg'\n",
    "style_reference_image_path = 'Pictures/abstract.jpg'\n",
    "\n",
    "width, height = load_img(target_image_path).size\n",
    "img_height = 400\n",
    "img_width = int(width * img_height / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19\n",
    "\n",
    "VGG19 model, with weights pre-trained on ImageNet\n",
    "Process:\n",
    "- Set up a network that will compute VGG19 layer activations for the style reference image, the target image, and the generated image at the same time.\n",
    "- Use the layer activations computed over these three images to define the loss function, which we will minimize in order to achieve style transfer.\n",
    "- Set up a gradient descent process to minimize this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height,img_width))\n",
    "    img = img_to_array(img) #Converts a PIL Image instance to a Numpy array.\n",
    "    img = np.expand_dims(img, axis=0) #Expand the shape of an array.\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 Network\n",
    "\n",
    "It takes as input a batch of three images: the style reference image, the target image, and a placeholder that will contain the generated image.\n",
    "The style reference and target image are static, and thus defined using K.constant, while the values contained in the placeholder of the generated image will change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rahuls98/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 131s 2us/step\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "\n",
    "combination_image = K.placeholder((1,img_height,img_width,3))\n",
    "\n",
    "input_tensor = K.concatenate([target_image, style_reference_image, combination_image], axis=0)\n",
    "\n",
    "model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.3-neural-style-transfer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
